Trying to get population figures on a range of locations
* Playground
  :PROPERTIES:
  :header-args: :noweb strip-exports
  :header-args:python: :python "pyenv exec poetry run python"
  :END:
** Census API
   Note that the environment variable  =CENSUS_GOV_API_KEY= should be set
  
   #+begin_src python :var api_key=(getenv "CENSUS_GOV_API_KEY") :results output
   import requests
  
   def get_population_census(city, state, api_key):
       url = f"https://api.census.gov/data/2019/pep/population?get=NAME,POP&for=place:*&in=state:*&key={api_key}"
       response = requests.get(url)
  
       if response.status_code != 200:
           print(f"Error fetching data from Census API: {response.status_code}")
           return None
  
       data = response.json()
       city_data = [row for row in data if city.lower() in row[0].lower() and state in row[0]]
  
       if city_data:
           population = city_data[0][1]
           return population
       else:
           return None
  
   cities_and_counties = [
       ("Northwest Arctic County", "AK"),
       ("Petersburg County", "AK"),
       ("Anchorage", "AK"),
       ("Bethel", "AK"),
       ("Fairbanks", "AK"),
       ("Juneau, City and Borough of", "AK"),
       ("Kenai", "AK"),
   ]
  
   api_key = "YOUR_API_KEY"  # Replace with your API key
  
   for city, state in cities_and_counties:
       population = get_population_census(city, state, api_key)
       if population:
           print(f"{city}, {state}: Population: {population}")
       else:
           print(f"{city}, {state}: Population data not found")
   #+end_src

 #+RESULTS:


 To be honest, the above doesn't work all that well. It does too manhy queries (that's fixable) but the data is in a format that doesn't quite match up with the input eg
 
 #+begin_src python :eval no
   [['Bethel city, Alaska', '6586', '02', '06520'],
    ['Kenai city, Alaska', '7807', '02', '38420']
    ['Juneau city and borough, Alaska', '31974', '02', '36400']]
 #+end_src

I could pull down data, but would have to do something smart for matching cities and counties

I *should* look into if there's actually a downloadable, open-source recommendation you can use here.

** Wikipedia

   Its searchable, thats the really nice thing
   #+begin_src python
   import wikipediaapi
  
   def get_population_wiki(city, lang="en"):
       wiki = wikipediaapi.Wikipedia(lang)
       page = wiki.page(city)
  
       if not page.exists():
           return None
  
       summary = page.summary.split("\n")
       population = None
  
       for line in summary:
           if "population" in line.lower():
               population = line
               break
  
       return population
  
  
   cities_and_counties = [
       ("Northwest Arctic County", "AK"),
       ("Petersburg County", "AK"),
       ("Anchorage", "AK"),
       ("Bethel", "AK"),
       ("Fairbanks", "AK"),
       ("Juneau, City and Borough of", "AK"),
       ("Kenai", "AK"),
   ]
  
   for city, state in cities_and_counties:
       city_state = f"{city}, {state}"
       population = get_population_wiki(city_state)
       if population:
           print(f"{city_state}: {population}")
       else:
           print(f"{city_state}: Population data not found")
   #+end_src

   #+RESULTS:
   : None
#+begin_src python :results code
  import requests
  def get_top_search_result(query):
      url = "https://en.wikipedia.org/w/api.php"
      params = {
          "action": "query",
          "list": "search",
          "format": "json",
          "srsearch": query,
          "utf8": 1,
          "formatversion": 2,
      }
  
      response = requests.get(url, params=params)
      data = response.json()
  
      return next(iter(data.get("query", {}).get("search", [])), None)
  
  return get_top_search_result("Bethel, AK")
#+end_src

#+RESULTS:
#+begin_src python
{'ns': 0, 'title': 'Bethel, Alaska', 'pageid': 105236, 'size': 36430, 'wordcount': 3079, 'snippet': '<span class="searchmatch">Bethel</span> (Central Yupik: Mamterilleq) is a city in <span class="searchmatch">Bethel</span> Census Area, Alaska, United States. It is the largest community on the Kuskokwim River, located', 'timestamp': '2023-02-24T20:26:54Z'}
#+end_src

** DBPedia
   
   Ok, heres the plan. DBPedia actually has this data in a structured format. *And* DBPedia has a way of doing fuzzy search using an endpoint like this https://lookup.dbpedia.org/api/search?query=Bethel,%20AK&maxResults=5
   
#+begin_src python :wrap "src xml"
  import requests
  import xml.dom.minidom
  search = "Bethel, Ak"
  url = f"https://lookup.dbpedia.org/api/search?query={search}&maxResults=2"
  response = requests.get(url)
  return xml.dom.minidom.parseString(response.text).toprettyxml()
#+end_src

#+RESULTS:
#+begin_src xml
<?xml version="1.0" ?>
<ArrayOfResults>
	<Result>
		<Label>Bethel Census Area, Alaska</Label>
		<URI>http://dbpedia.org/resource/Bethel_Census_Area,_Alaska</URI>
		<Description>Bethel Census Area (Russian: Бетел) is a census area in the U.S. state of Alaska. As of the 2010 census, the population is 17,013. It is part of the unorganized borough and therefore has no borough seat. Its largest community is the city of Bethel, which is also the largest city in the unorganized borough.</Description>
		<Classes>
			<Class>
				<Label>Administrative Region</Label>
				<URI>http://dbpedia.org/ontology/AdministrativeRegion</URI>
			</Class>
			<Class>
				<Label>Place</Label>
				<URI>http://dbpedia.org/ontology/Place</URI>
			</Class>
			<Class>
				<Label>Populated Place</Label>
				<URI>http://dbpedia.org/ontology/PopulatedPlace</URI>
			</Class>
			<Class>
				<Label>Location</Label>
				<URI>http://dbpedia.org/ontology/Location</URI>
			</Class>
			<Class>
				<Label>Region</Label>
				<URI>http://dbpedia.org/ontology/Region</URI>
			</Class>
		</Classes>
		<Categories>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Bethel_Census_Area,_Alaska</URI>
			</Category>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Bering_Sea</URI>
			</Category>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Alaska_census_areas</URI>
			</Category>
		</Categories>
		<Refcount>50</Refcount>
	</Result>
	<Result>
		<Label>Bethel, Alaska</Label>
		<URI>http://dbpedia.org/resource/Bethel,_Alaska</URI>
		<Description>Bethel (Mamterilleq in Central Alaskan Yup'ik; Russian: Бетел) is the largest community on the Kuskokwim River, located approximately 50 mi (80 km) upriver from where the river flows into Kuskokwim Bay. Bethel is the largest community in western Alaska and in the Unorganized Borough, as well as the ninth largest in the state, with a population of 6,080 as of the 2010 Census. Annual events in Bethel include the Kuskokwim 300, a dogsled race; Camai, a Yup'ik dance festival held each spring, and the Bethel Fair held in August.</Description>
		<Classes>
			<Class>
				<Label>Settlement</Label>
				<URI>http://dbpedia.org/ontology/Settlement</URI>
			</Class>
			<Class>
				<Label>City</Label>
				<URI>http://dbpedia.org/ontology/City</URI>
			</Class>
			<Class>
				<Label>Place</Label>
				<URI>http://dbpedia.org/ontology/Place</URI>
			</Class>
			<Class>
				<Label>Populated Place</Label>
				<URI>http://dbpedia.org/ontology/PopulatedPlace</URI>
			</Class>
			<Class>
				<Label>Location</Label>
				<URI>http://dbpedia.org/ontology/Location</URI>
			</Class>
		</Classes>
		<Categories>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Cities_in_Alaska</URI>
			</Category>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Cities_in_Unorganized_Borough,_Alaska</URI>
			</Category>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Cities_in_Bethel_Census_Area,_Alaska</URI>
			</Category>
			<Category>
				<URI>http://dbpedia.org/resource/Category:History_of_the_Alaska_Province_of_the_Moravian_Church</URI>
			</Category>
			<Category>
				<URI>http://dbpedia.org/resource/Category:Bethel,_Alaska</URI>
			</Category>
		</Categories>
		<Refcount>35</Refcount>
	</Result>
</ArrayOfResults>
#+end_src

Note that these search results Give you enough to both determine which is referring to a City and which is referring to a county (AdministrativeRegion) and the URI of each entity.

You can now get the population via a sparql query
#+begin_src python
  uri = 'http://dbpedia.org/resource/Bethel_Census_Area,_Alaska'
  from SPARQLWrapper import SPARQLWrapper, JSON
  sparql = SPARQLWrapper("http://dbpedia.org/sparql")
  sparql.setQuery(f"""
      PREFIX dbo: <http://dbpedia.org/ontology/>
  
      SELECT ?population WHERE {{
          <{uri}> dbo:populationTotal ?population .
      }}
  """)
  sparql.setReturnFormat(JSON)
  response = sparql.query().convert()
  bindings = response.get('results', {}).get('bindings', [])
  population = next((b['population']['value'] for b in bindings if 'population' in b), None)
  return population
#+end_src

#+RESULTS:
: 18666

In fact, it should be possible to write a sparql query to get multiple populations at the same time

It is!
#+begin_src python :results code
  from SPARQLWrapper import SPARQLWrapper, JSON
  resource_uri1 = "http://dbpedia.org/resource/Bethel_Census_Area,_Alaska"
  resource_uri2 = "http://dbpedia.org/resource/Bethel,_Alaska"
  sparql = SPARQLWrapper("http://dbpedia.org/sparql")
  sparql.setQuery(f"""
      PREFIX dbo: <http://dbpedia.org/ontology/>
  
      SELECT ?resource ?population WHERE {{
          VALUES ?resource {{ <{resource_uri1}> <{resource_uri2}> }}
          ?resource dbo:populationTotal ?population .
      }}
  """)
  sparql.setReturnFormat(JSON)
  response = sparql.query().convert()
  return response['results']['bindings']
#+end_src

#+RESULTS:
#+begin_src python
[{'resource': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Bethel,_Alaska'}, 'population': {'type': 'typed-literal', 'datatype': 'http://www.w3.org/2001/XMLSchema#nonNegativeInteger', 'value': '6325'}}, {'resource': {'type': 'uri', 'value': 'http://dbpedia.org/resource/Bethel_Census_Area,_Alaska'}, 'population': {'type': 'typed-literal', 'datatype': 'http://www.w3.org/2001/XMLSchema#nonNegativeInteger', 'value': '18666'}}]
#+end_src

Alright, so lets do this

* Code it
:PROPERTIES:
:header-args: :noweb strip-exports
:header-args:python: :python "pyenv exec poetry run python"
:header-args:python+: :session populations-of-locations
:header-args:python+: :results silent
:header-args:python+: :tangle ./population_of_locations.py
:header-args:python+: :comments link
:END:

** Prep    

Configure logging
#+begin_src python
  import os
  import logging
  import sys
  
  logging.basicConfig(
      level=os.environ.get("LOG_LEVEL", "INFO"),
      format="%(asctime)s - %(levelname)s - %(message)s",
      handlers=[logging.StreamHandler(sys.stdout)]
  )
#+end_src
And create a function to clear out output

#+begin_src python
  import shutil
  
  def reset_output_dir(output_dir="output"):
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    logging.debug("Output directory %s reset", output_dir)
    #+end_src
    
** Read in Input
    Here's our sample input
    #+begin_src csv :tangle minimal-sample-input.csv
8,AK,North Slope,County
14,AK,"Juneau, City and Borough of",City
9,AK,Northwest Arctic,County
10,AK,Petersburg,County
11,AK,Anchorage,City
12,AK,Bethel,City
13,AK,Fairbanks,City
    #+end_src
    
We don't want to do the whole thing at once, I'd like to have some intermediate output

 #+begin_src python
  import requests
  import csv
  from itertools import islice
  
  def get_csv_input_batches(input_filename='./minimal-sample-input.csv', batch_size=100, skip=0, take=None):
      csv_read_done = object()
      with open(input_filename, 'r') as input_file:
          csv_line = csv.reader(input_file)
          try:
              for _ in range(skip):
                  next(csv_line)
          except StopIteration:
              return
          number_processed = 0
          while True:
              res = []
              for _ in range(batch_size):
                  try:
                      res.append(next(csv_line))
                      number_processed += 1
                      if take is not None and number_processed >= take:
                          yield res
                          return
                  except StopIteration:
                      yield res
                      return
              yield res
 #+end_src

Lets try it out

#+begin_src python :tangle no :results replace code :wrap "src python :tangle no"
  list(get_csv_input_batches(batch_size=3))
#+end_src

#+RESULTS:
#+begin_src python :tangle no
[[['8', 'AK', 'North Slope', 'County'], ['14', 'AK', 'Juneau, City and Borough of', 'City'], ['9', 'AK', 'Northwest Arctic', 'County']], [['10', 'AK', 'Petersburg', 'County'], ['11', 'AK', 'Anchorage', 'City'], ['12', 'AK', 'Bethel', 'City']], [['13', 'AK', 'Fairbanks', 'City']]]
#+end_src

Ok, so that works, lets do the rest of it I guess

** For each batch
#+begin_src python
  def process_batches(batches, output_file=None):
    for rows in batches:
      logging.debug('Handling batch of %s rows', len(rows))
      identifiers_uris = get_uris_for_batch(rows)
      identifier_populations = find_population_facts(identifiers_uris)
      write_to_output(rows, identifier_populations, output_file=output_file)
#+end_src
*** For each row
#+begin_src python 
  def get_uris_for_batch(rows):
    for identifier, state_code, name, city_county in rows:
      found_results = search_for_term(f"{name}, {state_code}")
      uri = best_matching_uri(found_results, city_county, identifier)
      if not uri is None:
        yield identifier, uri
#+end_src

**** Search for possible URIs
#+begin_src python
  import requests
  from xml.etree import ElementTree 
  
  def search_for_term(term):
    url = f"https://lookup.dbpedia.org/api/search?query={term}&maxResults=5"
    response = requests.get(url)
    return ElementTree.fromstring(response.content)
#+end_src
**** Identify best matching URI
This depends on which are tagged City or County
#+begin_src python
  def best_matching_uri(tree, city_county, identifier):
      if city_county == "City":
          search_class_uri = ("http://dbpedia.org/ontology/City",)
      elif city_county == "County":
          search_class_uri = ("http://dbpedia.org/ontology/Region", "http://dbpedia.org/ontology/AdministrativeRegion")
      else:
          logging.error("Unknown city or county %s", city_county)
          return None
  
      for result in tree.findall("Result"):
          classes = result.find("Classes")
          for class_element in classes.findall("Class"):
              if class_element.find("URI").text in search_class_uri:
                  return result.find("URI").text
  
      logging.error("No appropriate uri for %s found. Searched %s", identifier, ElementTree.tostring(tree,encoding='unicode'))
      return None
#+end_src
Test it

#+begin_src python :tangle no :results replace code :wrap "src python :tangle no"
  rows = next(iter(get_csv_input_batches(batch_size=3)))
  identifier_uris = list(get_uris_for_batch(rows))
#+end_src

#+RESULTS:
#+begin_src python :tangle no
#+end_src

*** Search for population facts on all URIs
#+begin_src python
  from SPARQLWrapper import SPARQLWrapper, JSON
  
  def find_population_facts(identifiers_uris_collection):
    sparql = SPARQLWrapper("http://dbpedia.org/sparql")
    identifiers_uris = list(identifiers_uris_collection)
    formatted_uris = (f'<{uri}>' for _, uri in identifiers_uris if uri)
    query = f"""
        PREFIX dbo: <http://dbpedia.org/ontology/>
        PREFIX dbp: <http://dbpedia.org/property/>
  
        SELECT ?resource ?population WHERE {{
            VALUES ?resource {{ {' '.join(formatted_uris)} }}
            {{
                ?resource dbo:populationTotal ?population .
            }} UNION {{
                ?resource dbp:populationTotal ?population .
            }}
        }}
    """
    sparql.setQuery(query)
    sparql.setReturnFormat(JSON)
    logging.debug("Using SPARQL query %s", query)
    response = sparql.query().convert()
    bindings = response['results']['bindings']
  
    for identifier, uri in identifiers_uris:
      population = next(
        (r.get('population', {}).get('value', None)
         for r in bindings
         if r.get('resource', {}).get('value', None) == uri
         ), None)
      if population is not None:
        yield identifier, int(population)
      else:
        logging.debug('Failed to resolve population for %s', identifier)
  
#+end_src
Test it

#+begin_src python :tangle no :results replace code :wrap "src python :tangle no"
  identifier_populations = list(find_population_facts(identifier_uris))
  identifier_populations
#+end_src

#+RESULTS:
#+begin_src python :tangle no
[('8', 11031), ('14', 32255), ('9', 7793)]
#+end_src

*** Append output CSV
#+begin_src python
  from collections import defaultdict
  default_output_file = './output/with_populations.csv'
  def write_to_output(rows, identifier_populations_collection, output_file=default_output_file):
      output_file = output_file or default_output_file
  
      identifier_populations = {}
      for identifier, population in identifier_populations_collection:
          if identifier not in identifier_populations:
            identifier_populations[identifier] = population
      logging.debug('Writing populations: %s', identifier_populations)
      
      num_resolved = defaultdict(lambda: 0)
      with open(output_file, 'a') as output_file:
          output = csv.writer(output_file)
          for identifier, *others in rows:
              population = identifier_populations.get(identifier, None)
              num_resolved[population is not None] += 1
              output.writerow([identifier, *others, population])
      logging.info("Resolved population for %s locations, failed to resolve for %s", num_resolved[True], num_resolved[False])
  
#+end_src
We can then try it out

#+begin_src python :tangle no 
  write_to_output(rows, identifier_populations)
#+end_src
And that looks like this
#+begin_src shell
  cat ./output/with_populations.csv
#+end_src

#+RESULTS:
|  8 | AK | North Slope                 | County | 11031 |
| 14 | AK | Juneau, City and Borough of | City   | 32255 |
|  9 | AK | Northwest Arctic            | County |  7793 |

Nice!
** Launcher
   #+begin_src python
  import argparse
  
  parser = argparse.ArgumentParser(description="Process location populations")
  parser.add_argument("--batch-size", type=int, default=50, help="Number of location populations to resolve at a time")
  parser.add_argument("--input-file", type=str, default="./minimal-sample-input.csv", help="Path to input CSV file")
  parser.add_argument("--output-file", type=str, default="./output/with_population.csv", help="Path to write output CSV file")
  parser.add_argument("--skip", type=int, default=0, help="Number of rows to skip in the input CSV file")
  parser.add_argument("--take", type=int, default=None, help="Number of rows to process in the input CSV file")
  parser.add_argument("--reset-output", type=bool, default=True, help="Reset the output directory by deleting contents and recreating")
  
  if __name__ == '__main__': 
    args = parser.parse_args()
    logging.debug('Start processing')
  
    if args.reset_output:
      reset_output_dir()
  
    batches = get_csv_input_batches(input_filename=args.input_file,
                                    batch_size=args.batch_size,
                                    skip=args.skip,
                                    take=args.take,)
    process_batches(batches, output_file=args.output_file)
   #+end_src
* Try it out
  #+begin_src emacs-lisp :results silent
  (async-shell-command "LOG_LEVEL=DEBUG pyenv exec poetry run python ./population_of_locations.py --input-file ./data/all-data.csv --batch-size 10 --take 3")
  #+end_src

Nice. Now as far as timing

  #+begin_src shell :results output
  time pyenv exec poetry run python ./population_of_locations.py --input-file ./data/all-data.csv --batch-size 20 --take 50
  #+end_src

#+RESULTS:
: 2023-03-26 14:57:28,919 - INFO - Resolved population for 20 locations, failed to resolve for 0
: 2023-03-26 14:57:50,525 - INFO - Resolved population for 20 locations, failed to resolve for 0
: 2023-03-26 14:58:03,828 - INFO - Resolved population for 10 locations, failed to resolve for 0

Well I'm not sure why the ~time~ command didn't work but we're seeing slightly less than 1 lookup per second. Sounds good to me. So the full script might take about 45 minutes to run.
